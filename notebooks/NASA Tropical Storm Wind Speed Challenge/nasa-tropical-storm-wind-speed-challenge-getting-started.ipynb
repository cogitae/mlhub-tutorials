{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "How to use the Radiant MLHub API to browse and download the NASA Tropical Storm Wind Speed Competition Data\n",
    "=====\n",
    "\n",
    "This Jupyter notebook, which you may copy and adapt for any use, shows basic examples of how to use the API to download labels and source imagery for the NASA Tropical Storm Wind Speed Competition dataset. Full documentation for the API is available at [docs.mlhub.earth](http://docs.mlhub.earth).\n",
    "\n",
    "We'll show you how to set up your authorization,retrieve the items (the data contained within them) from those collections, and load the data into a dataframe.\n",
    "\n",
    "Each item in our collection is explained in json format compliant with STAC label extension definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation\n",
    "====\n",
    "M. Maskey, R. Ramachandran, I. Gurung, B. Freitag, M. Ramasubramanian, J. Miller (2020) \"Tropical Cyclone Wind Estimation Competition Dataset\", Version 1.0, Radiant MLHub. \\[Date Accessed\\] [https://doi.org/10.34911/rdnt.xs53up](https://doi.org/10.34911/rdnt.xs53up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authentication\n",
    "-----\n",
    "\n",
    "Access to the Radiant MLHub API requires an API key. To get your API key, go to [dashboard.mlhub.earth](https://dashboard.mlhub.earth). If you have not used Radiant MLHub before, you will need to sign up and create a new account. Otherwise, sign in. In the **API Keys** tab, you'll be able to create API key, which you will need. *Do not share* your API key with others: your usage may be limited and sharing your API key is a security risk.\n",
    "\n",
    "Copy the API key, and paste it in the box bellow.\n",
    "\n",
    "The Collection ID for the labels of the LandCoverNet dataset can be found on the [registry page](https://registry.mlhub.earth/10.34911/rdnt.d2ce8i). We will save that to a variable which will be used during API requests.\n",
    "\n",
    "Click **Run** or press SHIFT + ENTER before moving on to run this first piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# copy your API key from dashboard.mlhub.earth and paste it in the following\n",
    "API_KEY = 'PASTE_YOUR_API_KEY_HERE'\n",
    "API_BASE = 'https://api.radiant.earth/mlhub/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Items Setup\n",
    "====\n",
    "The code below sets up functions which we will use to download items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import arrow\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from tqdm import tqdm\n",
    "    \n",
    "def download_http(uri, path, retries=0):\n",
    "    if retries >= 3:\n",
    "        return\n",
    "    try:\n",
    "        parsed = urlparse(uri)\n",
    "        r = requests.get(uri)\n",
    "        file_path = os.path.join(path, parsed.path.split('/')[-1])\n",
    "        if os.path.exists(file_path):\n",
    "            return\n",
    "        f = open(file_path, 'wb')\n",
    "        for chunk in r.iter_content(chunk_size=512 * 1024): \n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "        f.close()\n",
    "    except:\n",
    "        download_http(uri, path, retries+1)\n",
    "\n",
    "def get_download_uri(uri):\n",
    "    r = requests.get(uri, allow_redirects=False)\n",
    "    return r.headers['Location']\n",
    "\n",
    "def download(d):\n",
    "    href = d[0]\n",
    "    path = d[1]\n",
    "    download_uri = get_download_uri(href)\n",
    "    parsed = urlparse(download_uri)\n",
    "    \n",
    "    if parsed.scheme in ['s3']:\n",
    "        download_s3(download_uri, path)\n",
    "    elif parsed.scheme in ['http', 'https']:\n",
    "        download_http(download_uri, path)\n",
    "        \n",
    "def get_source_item_assets(args):\n",
    "    path = args[0]\n",
    "    href = args[1]\n",
    "    asset_downloads = []\n",
    "    try:\n",
    "        r = requests.get(href, params={'key': API_KEY})\n",
    "    except:\n",
    "        print('ERROR: Could Not Load', href)\n",
    "        return []\n",
    "    asset_path = path\n",
    "    if not os.path.exists(asset_path):\n",
    "        os.makedirs(asset_path)\n",
    "\n",
    "    for key, asset in r.json()['assets'].items():\n",
    "        asset_downloads.append((asset['href'], asset_path))\n",
    "        \n",
    "    return asset_downloads\n",
    "\n",
    "def get_test_source_item_assets(item):\n",
    "    asset_downloads = []\n",
    "    asset_path = f'test/{item[\"id\"].split(\"_\")[-2]}/'\n",
    "    if not os.path.exists(asset_path):\n",
    "        os.makedirs(asset_path)\n",
    "\n",
    "    for key, asset in item['assets'].items():\n",
    "        asset_downloads.append([(asset['href'], asset_path)])\n",
    "        \n",
    "    return asset_downloads\n",
    "\n",
    "def download_source_and_labels(item):\n",
    "    labels = item.get('assets').get('labels')\n",
    "    links = item.get('links')\n",
    "    \n",
    "    # Make the directory to download the files to\n",
    "    path = f'train/{item[\"id\"].split(\"_\")[-2]}/'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    source_items = []\n",
    "    \n",
    "    # Download the source imagery\n",
    "    for link in links:\n",
    "        if link['rel'] != 'source':\n",
    "            continue\n",
    "        source_items.append((path, link['href']))\n",
    "        \n",
    "    results = list(map(get_source_item_assets, source_items))\n",
    "    results.append([(labels['href'], path)])\n",
    "            \n",
    "    return results\n",
    "\n",
    "def download_train_items(uri=None):\n",
    "    if uri is None:\n",
    "        uri = f'{API_BASE}/collections/nasa_tropical_storm_competition_train_labels/items'\n",
    "    print('Loading', uri, '...')\n",
    "    r = requests.get(uri, params={'key': API_KEY})\n",
    "    collection = r.json()\n",
    "    for feature in collection.get('features', []):\n",
    "        for d in download_source_and_labels(feature):\n",
    "            for args in d:\n",
    "                download(args)\n",
    "    \n",
    "        \n",
    "    # Get the next page if results, if available\n",
    "    for link in collection.get('links', []):\n",
    "        if link['rel'] == 'next' and link['href'] is not None:\n",
    "            download_train_items(link['href'])\n",
    "\n",
    "def download_test_items(uri=None):\n",
    "    if uri is None:\n",
    "        uri = f'{API_BASE}/collections/nasa_tropical_storm_competition_test_source/items'\n",
    "    print('Loading', uri, '...')\n",
    "    r = requests.get(uri, params={'key': API_KEY})\n",
    "    collection = r.json()\n",
    "    for feature in collection.get('features', []):\n",
    "        for d in get_test_source_item_assets(feature):\n",
    "            for args in d:\n",
    "                download(args)\n",
    "        \n",
    "    for link in collection.get('links', []):\n",
    "        if link['rel'] == 'next' and link['href'] is not None:\n",
    "            download_test_items(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Training Items\n",
    "=====\n",
    "Run the cell below to download the train items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_train_items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Test Items\n",
    "====\n",
    "Run the cell below to download the test items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_test_items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data into a Dataframe\n",
    "====\n",
    "The below code will load both the training and test items into dataframes and sort the rows by the Image ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "files = glob.glob('train/**/*.jpg')\n",
    "pd_data = []\n",
    "\n",
    "for fname in files:\n",
    "    label_file = fname.replace('.jpg', '_label.json')\n",
    "    features_file = fname.replace('.jpg', '_features.json')\n",
    "    storm_id = fname.split('/')[1]\n",
    "    image_id = fname.split('/')[-1].replace('.jpg', '')\n",
    "    \n",
    "    with open(features_file, 'r') as f:\n",
    "        features_data = json.load(f)\n",
    "        \n",
    "    with open(label_file, 'r') as f:\n",
    "        label_data = json.load(f)\n",
    "        \n",
    "    pd_data.append([image_id, storm_id, int(features_data['relative_time']), int(features_data['ocean']), int(label_data['wind_speed']), fname])\n",
    "\n",
    "train_df = pd.DataFrame(np.array(pd_data),\n",
    "                   columns=['Image ID', 'Storm ID', 'Relative Time', 'Ocean', 'Wind Speed', 'Image File Path']).sort_values(by=['Image ID'])\n",
    "\n",
    "files = glob.glob('test/**/*.jpg')\n",
    "pd_data = []\n",
    "\n",
    "for fname in files:\n",
    "    features_file = fname.replace('.jpg', '_features.json')\n",
    "    storm_id = fname.split('/')[1]\n",
    "    image_id = fname.split('/')[-1].replace('.jpg', '')\n",
    "    \n",
    "    with open(features_file, 'r') as f:\n",
    "        features_data = json.load(f)\n",
    "        \n",
    "    with open(label_file, 'r') as f:\n",
    "        label_data = json.load(f)\n",
    "        \n",
    "    pd_data.append([image_id, storm_id, int(features_data['relative_time']), int(features_data['ocean']), fname])\n",
    "\n",
    "test_df = pd.DataFrame(np.array(pd_data),\n",
    "                   columns=['Image ID', 'Storm ID', 'Relative Time', 'Ocean', 'Image File Path']).sort_values(by=['Image ID'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
